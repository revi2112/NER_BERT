{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e46ac6cc",
      "metadata": {
        "id": "e46ac6cc"
      },
      "source": [
        "# Named Entity Recognition with BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "255b419e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "255b419e",
        "outputId": "56585aa9-a9b1-43b4-a92e-74461219f6c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers wordcloud scikit-learn pandas matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4191098b",
      "metadata": {
        "id": "4191098b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def56c72",
      "metadata": {
        "id": "def56c72"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").ffill()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j1mR1iKLQPqj",
      "metadata": {
        "id": "j1mR1iKLQPqj"
      },
      "outputs": [],
      "source": [
        "print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rBOvzcOXvHV5",
      "metadata": {
        "id": "rBOvzcOXvHV5"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MbtxbjkBvKfj",
      "metadata": {
        "id": "MbtxbjkBvKfj"
      },
      "outputs": [],
      "source": [
        "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n",
        "df = df.dropna(subset=['Word'])\n",
        "\n",
        "df.isnull().sum()\n",
        "\n",
        "print(f\"Total sentences: {df['Sentence #'].nunique()}\")\n",
        "print(f\"Total tokens: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c36721",
      "metadata": {
        "id": "03c36721"
      },
      "outputs": [],
      "source": [
        "sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).tolist()\n",
        "tags = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).tolist()\n",
        "\n",
        "unique_tags = sorted(set(tag for seq in tags for tag in seq))\n",
        "label2id = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "id2label = {idx: tag for tag, idx in label2id.items()}\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, tags, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JcFctm3SvVZf",
      "metadata": {
        "id": "JcFctm3SvVZf"
      },
      "outputs": [],
      "source": [
        "# Unique NER Tags\n",
        "ner_tags = df['Tag'].unique()\n",
        "print(\"NER Tags:\", ner_tags)\n",
        "\n",
        "# Unique POS Tags\n",
        "pos_tags = df['POS'].unique()\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "print(\"Number of NER Tags:\", len(ner_tags))\n",
        "print(\"Number of POS Tags:\", len(pos_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9ca721",
      "metadata": {
        "id": "9e9ca721"
      },
      "outputs": [],
      "source": [
        "print(\"Label to ID mapping:\")\n",
        "print(label2id)\n",
        "print(\"\\nID to Label mapping:\")\n",
        "print(id2label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1b58f8",
      "metadata": {
        "id": "dd1b58f8"
      },
      "outputs": [],
      "source": [
        "class NERDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, label2id, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        encoding = self.tokenizer(words, is_split_into_words=True, return_offsets_mapping=True,\n",
        "                                  padding=\"max_length\", truncation=True, max_length=self.max_len)\n",
        "        offset_mapping = encoding.pop(\"offset_mapping\")\n",
        "        word_ids = encoding.word_ids()\n",
        "        aligned_labels = np.ones(len(encoding[\"input_ids\"]), dtype=int) * -100\n",
        "        previous_word_idx = None\n",
        "        for i, word_idx in enumerate(word_ids):\n",
        "            if word_idx is None:\n",
        "                continue\n",
        "            if word_idx != previous_word_idx:\n",
        "                aligned_labels[i] = self.label2id.get(labels[word_idx], 0)\n",
        "            previous_word_idx = word_idx\n",
        "        item = {key: torch.tensor(val) for key, val in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(aligned_labels)\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e8b315",
      "metadata": {
        "id": "02e8b315"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_tags),\n",
        ").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f98694c1",
      "metadata": {
        "id": "f98694c1"
      },
      "outputs": [],
      "source": [
        "train_dataset = NERDataset(train_texts, train_labels, tokenizer, label2id)\n",
        "val_dataset = NERDataset(val_texts, val_labels, tokenizer, label2id)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3bfc59",
      "metadata": {
        "id": "dd3bfc59"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=0, num_training_steps=len(train_loader)*5)\n",
        "\n",
        "train_losses, val_f1s = [], []\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    model.eval()\n",
        "    preds, trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            pred = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            true = batch[\"labels\"].cpu().numpy()\n",
        "            for p, t in zip(pred, true):\n",
        "                for pi, ti in zip(p, t):\n",
        "                    if ti != -100:\n",
        "                        preds.append(id2label[pi])\n",
        "                        trues.append(id2label[ti])\n",
        "    f1 = f1_score(trues, preds, average=\"weighted\")\n",
        "    val_f1s.append(f1)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f}, Validation F1 = {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52da589c",
      "metadata": {
        "id": "52da589c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(val_f1s, label=\"Validation F1\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.legend()\n",
        "plt.title(\"Training Loss and Validation F1\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9296b962",
      "metadata": {
        "id": "9296b962"
      },
      "outputs": [],
      "source": [
        "# Run inference on our custom sentence\n",
        "tokens = [\"Elon\", \"Musk\", \"is\", \"the\", \"CEO\", \"of\", \"Tesla\", \"and\", \"SpaceX\", \"based\", \"in\", \"the\", \"United\", \"States\"]\n",
        "inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "outputs = model(**inputs).logits\n",
        "predictions = torch.argmax(outputs, dim=2)[0].cpu().numpy()\n",
        "predicted_labels = [id2label[p] for p in predictions]\n",
        "print(\"Predicted NER Tags:\")\n",
        "print(list(zip(tokens, predicted_labels)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a26907",
      "metadata": {
        "id": "c7a26907"
      },
      "outputs": [],
      "source": [
        "text = \" \".join(df[df['Tag'] != 'O']['Word'].values)\n",
        "wordcloud = WordCloud(width=1000, height=600, background_color=\"white\", colormap=\"viridis\").generate(text)\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud of Named Entities\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FtMPY9u5Xr_H",
      "metadata": {
        "collapsed": true,
        "id": "FtMPY9u5Xr_H"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Installing seqeval\n",
        "!pip install transformers datasets seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W5BhL0KQX3_T",
      "metadata": {
        "collapsed": true,
        "id": "W5BhL0KQX3_T"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "from datasets import DatasetDict, Dataset  # only import DatasetDict and Dataset here\n",
        "!pip install evaluate\n",
        "import evaluate # for importing metrics, this is the correct import.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pmGb24pdX9Gj",
      "metadata": {
        "id": "pmGb24pdX9Gj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loading and preprocessing the dataset\n",
        "df = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
        "df = df.fillna(method=\"ffill\")\n",
        "\n",
        "# Creating the grouped sentences and labels\n",
        "grouped = df.groupby(\"Sentence #\").agg({\"Word\": list, \"Tag\": list}).reset_index()\n",
        "sentences = grouped[\"Word\"].tolist()\n",
        "labels = grouped[\"Tag\"].tolist()\n",
        "\n",
        "# Unique labels for the dataset\n",
        "label_list = sorted(set(tag for seq in labels for tag in seq))\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "# Encoding labels to IDs\n",
        "encoded_labels = [[label2id[tag] for tag in seq] for seq in labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75uDYyi23yGo",
      "metadata": {
        "id": "75uDYyi23yGo"
      },
      "outputs": [],
      "source": [
        "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')\n",
        "sent_lengths = df.groupby(\"Sentence #\")['Word'].count()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(sent_lengths, bins=50, kde=True)\n",
        "plt.title('Sentence Length Distribution')\n",
        "plt.xlabel('Number of Tokens')\n",
        "plt.ylabel('Number of Sentences')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uXp7coYj34Dk",
      "metadata": {
        "id": "uXp7coYj34Dk"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "entity_words = df[df['Tag'] != 'O']['Word']\n",
        "entity_counts = Counter(entity_words)\n",
        "\n",
        "# Top 20 named entities\n",
        "most_common_entities = entity_counts.most_common(20)\n",
        "words, counts = zip(*most_common_entities)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=list(words), y=list(counts))\n",
        "plt.title('Top 20 Named Entities')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Entity Word')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8-xzzxG638ZM",
      "metadata": {
        "id": "8-xzzxG638ZM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "top_pos = df['POS'].value_counts().head(20)\n",
        "sns.barplot(x=top_pos.index, y=top_pos.values)\n",
        "plt.title('Top 20 POS Tags')\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('POS Tag')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u9Y6pgoAZ71n",
      "metadata": {
        "id": "u9Y6pgoAZ71n"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tokenize inputs\n",
        "model_name = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", is_split_into_words=True, max_length=128)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[i][word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MLzb_SJ_Z7oy",
      "metadata": {
        "id": "MLzb_SJ_Z7oy"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Prepare Hugging Face dataset\n",
        "dataset = Dataset.from_dict({\n",
        "    \"tokens\": sentences,\n",
        "    \"ner_tags\": encoded_labels\n",
        "})\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "print(\"manee-checking for the fuctionality\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac095aff",
      "metadata": {
        "id": "ac095aff"
      },
      "outputs": [],
      "source": [
        "# Check sample label encoding\n",
        "print(\"Sample original tags:\", tags[0])\n",
        "print(\"Encoded tags:\", encoded_labels[0])\n",
        "print(\"Back to label names:\", [id2label[i] for i in encoded_labels[0]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u1lGOJEVaof1",
      "metadata": {
        "id": "u1lGOJEVaof1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vvNTw_1Z7cR",
      "metadata": {
        "id": "8vvNTw_1Z7cR"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define label mappings from string tags to integers\n",
        "label_list = sorted(set(tag for seq in tags for tag in seq))\n",
        "label2id = {l: i for i, l in enumerate(label_list)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "# Encoding string tags to integer IDs\n",
        "encoded_labels = [[label2id[tag] for tag in seq] for seq in tags]\n",
        "\n",
        "# Spliting into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    sentences, encoded_labels, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# Building Hugging Face dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\"tokens\": train_texts, \"ner_tags\": train_labels}),\n",
        "    \"test\": Dataset.from_dict({\"tokens\": val_texts, \"ner_tags\": val_labels}),\n",
        "})\n",
        "\n",
        "# Tokenization and label alignment function\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the tokenization and label alignment\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# Convert to PyTorch DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=16, shuffle=True)\n",
        "eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AX0wQHQocoKa",
      "metadata": {
        "id": "AX0wQHQocoKa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Saving model and testing\n",
        "model.save_pretrained(\"bert-ner-model\")\n",
        "tokenizer.save_pretrained(\"bert-ner-model\")\n",
        "\n",
        "# Example prediction\n",
        "test_sentence = \"Elon Musk is the CEO of Tesla and SpaceX, based in the United States.\"\n",
        "''' We have taken a sample sentence for our prediction. If you want to try more,\n",
        "here are some of the random sentences for prediction testing'''\n",
        "# \"Elon Musk mowa is the CEO of Tesla and SpaceX, based in the United States.\"\n",
        "# \"Mr. Mohan Das Karamchand Gandhi was the person behind betrayal of electing Prime Minister of India\"\n",
        "# \"Steve Jobs founded Apple in California.\"\n",
        "# \"Madam Marie Curie won the Nobel Prize for her work in radioactivity.\"\n",
        "# \"The Amazon River flows through Brazil and Peru.\"\n",
        "# \"Christopher Nolan directed Inception which was released in 2010.\"\n",
        "# \"Lionel Messi joined Inter Miami after leaving Paris Saint-Germain.\"\n",
        "# \"Harvard University is located in Cambridge, Massachusetts.\"\n",
        "\n",
        "\n",
        "tokens = tokenizer(test_sentence.split(), return_tensors=\"pt\", is_split_into_words=True)\n",
        "with torch.no_grad():\n",
        "    output = model(**tokens)\n",
        "logits = output.logits\n",
        "predictions = torch.argmax(logits, dim=-1)\n",
        "predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "for token, label in zip(test_sentence.split(), predicted_labels[1:len(test_sentence.split())+1]):\n",
        "    print(f\"{token:15s} --> {label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zNfVFCXcH2L1",
      "metadata": {
        "id": "zNfVFCXcH2L1"
      },
      "source": [
        "# distibert-base-cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CdqyeR0TH7WV",
      "metadata": {
        "collapsed": true,
        "id": "CdqyeR0TH7WV"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "distilbert_model_name = \"distilbert-base-cased\"\n",
        "distilbert_tokenizer = DistilBertTokenizerFast.from_pretrained(distilbert_model_name)\n",
        "\n",
        "#tokenize inpus\n",
        "def tokenize_and_align_labels_distilbert(examples, label_all_tokens = False):\n",
        "    tokenized_inputs = distilbert_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_distil = dataset.map(tokenize_and_align_labels_distilbert, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wW-_p00pPg89",
      "metadata": {
        "collapsed": true,
        "id": "wW-_p00pPg89"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForTokenClassification\n",
        "\n",
        "distilbert_model = DistilBertForTokenClassification.from_pretrained(\n",
        "    distilbert_model_name,\n",
        "    num_labels=len(label2id),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AFGgj7t02vYP",
      "metadata": {
        "id": "AFGgj7t02vYP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xzA3d0-lPj4t",
      "metadata": {
        "id": "xzA3d0-lPj4t"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=distilbert_tokenizer)\n",
        "\n",
        "train_dataloader_distil = DataLoader(train_dataset, batch_size=32, collate_fn=data_collator)\n",
        "val_dataloader_distil = DataLoader(val_dataset, batch_size=32, collate_fn=data_collator)\n",
        "\n",
        "\n",
        "train_dataset_distil = tokenized_distil[\"train\"]\n",
        "val_dataset_distil = tokenized_distil[\"test\"]\n",
        "\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = AdamW(distilbert_model.parameters(), lr=5e-5)\n",
        "\n",
        "num_training_steps = len(train_dataloader_distil) * 10\n",
        "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PHvB340F4-HZ",
      "metadata": {
        "id": "PHvB340F4-HZ"
      },
      "outputs": [],
      "source": [
        "#Training for DistilBert\n",
        "from tqdm.auto import tqdm\n",
        "train_losses, val_f1s = [], []\n",
        "\n",
        "for epoch in range(10):\n",
        "  distilbert_model.train()\n",
        "  total_loss = 0\n",
        "  for batch in tqdm(train_dataloader_distil, desc=f\"DistilBERT Epoch {epoch+1}\"):\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      if 'token_type_ids' in batch:\n",
        "        del batch['token_type_ids']\n",
        "      outputs = distilbert_model(**batch)\n",
        "      loss = outputs.loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      lr_scheduler.step()\n",
        "      optimizer.zero_grad()\n",
        "      total_loss += loss.item()\n",
        "      avg_loss = total_loss / len(train_dataloader_distil)\n",
        "  train_losses.append(avg_loss)\n",
        "  distilbert_model.eval()\n",
        "  distil_preds, distil_trues = [], []\n",
        "  with torch.no_grad():\n",
        "      for batch in val_dataloader_distil:\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "          if 'token_type_ids' in batch:\n",
        "            del batch['token_type_ids']\n",
        "          outputs = distilbert_model(**batch)\n",
        "          pred = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "          true = batch[\"labels\"].cpu().numpy()\n",
        "          for p, t in zip(pred, true):\n",
        "              for pi, ti in zip(p, t):\n",
        "                  if ti != -100:\n",
        "                      distil_preds.append(id2label[pi])\n",
        "                      distil_trues.append(id2label[ti])\n",
        "  distil_f1 = f1_score(distil_trues, distil_preds, average=\"weighted\")\n",
        "  val_f1s.append(distil_f1)\n",
        "  print(f\"[DistilBERT] Epoch {epoch+1}: Train Loss = {avg_loss:.4f}, Validation F1 = {distil_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cQiLIcJvQbff",
      "metadata": {
        "id": "cQiLIcJvQbff"
      },
      "outputs": [],
      "source": [
        "# Saving model and testing\n",
        "model.save_pretrained(\"distilbert-bert-ner-model\")\n",
        "tokenizer.save_pretrained(\"distilbert-bert-ner-model\")\n",
        "\n",
        "# Example prediction\n",
        "test_sentence = \"Elon Musk is the CEO of Tesla and SpaceX, based in the United States.\"\n",
        "''' We have taken a sample sentence for our prediction. If you want to try more,\n",
        "here are some of the random sentences for prediction testing'''\n",
        "# \"Elon Musk mowa is the CEO of Tesla and SpaceX, based in the United States.\"\n",
        "# \"Mr. Mohan Das Karamchand Gandhi was the person behind betrayal of electing Prime Minister of India\"\n",
        "# \"Steve Jobs founded Apple in California.\"\n",
        "# \"Madam Marie Curie won the Nobel Prize for her work in radioactivity.\"\n",
        "# \"The Amazon River flows through Brazil and Peru.\"\n",
        "# \"Christopher Nolan directed Inception which was released in 2010.\"\n",
        "# \"Lionel Messi joined Inter Miami after leaving Paris Saint-Germain.\"\n",
        "# \"Harvard University is located in Cambridge, Massachusetts.\"\n",
        "\n",
        "\n",
        "tokens = tokenizer(test_sentence.split(), return_tensors=\"pt\", is_split_into_words=True)\n",
        "with torch.no_grad():\n",
        "    output = model(**tokens)\n",
        "logits = output.logits\n",
        "predictions = torch.argmax(logits, dim=-1)\n",
        "predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "for token, label in zip(test_sentence.split(), predicted_labels[1:len(test_sentence.split())+1]):\n",
        "    print(f\"{token:15s} --> {label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5690e892",
      "metadata": {
        "id": "5690e892"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Multilingual BERT Implementation (bert-base-multilingual-cased)\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "from seqeval.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "\n",
        "# Tokenizer & model\n",
        "tokenizer_multi = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model_multi = BertForTokenClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=len(label_list))\n",
        "\n",
        "# Tokenize and align labels again for multilingual model\n",
        "def tokenize_and_align_labels_multilingual(examples):\n",
        "    tokenized_inputs = tokenizer_multi(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=\"max_length\", max_length=128\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Prepare multilingual dataset\n",
        "tokenized_multi_dataset = dataset.map(tokenize_and_align_labels_multilingual, batched=True)\n",
        "train_test_multi = tokenized_multi_dataset.train_test_split(test_size=0.2)\n",
        "train_multi = train_test_multi[\"train\"]\n",
        "eval_multi = train_test_multi[\"test\"]\n",
        "\n",
        "# Compute metrics\n",
        "def compute_metrics_multilingual(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_preds, true_labels = [], []\n",
        "    for pred, label in zip(predictions, labels):\n",
        "        preds, labs = [], []\n",
        "        for p, l in zip(pred, label):\n",
        "            if l != -100:\n",
        "                preds.append(id2label[p])\n",
        "                labs.append(id2label[l])\n",
        "        true_preds.append(preds)\n",
        "        true_labels.append(labs)\n",
        "    print(classification_report(true_labels, true_preds))\n",
        "    return {\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "    }\n",
        "\n",
        "# TrainingArguments\n",
        "training_args_multi = TrainingArguments(\n",
        "    output_dir=\"./results-multilingual\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer_multi = Trainer(\n",
        "    model=model_multi,\n",
        "    args=training_args_multi,\n",
        "    train_dataset=train_multi,\n",
        "    eval_dataset=eval_multi,\n",
        "    tokenizer=tokenizer_multi,\n",
        "    compute_metrics=compute_metrics_multilingual\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer_multi.train()\n",
        "trainer_multi.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# bert-base-multilingual-cased"
      ],
      "metadata": {
        "id": "2LAHsUuIX4lm"
      },
      "id": "2LAHsUuIX4lm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472e924a",
      "metadata": {
        "id": "472e924a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install transformers datasets seqeval\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "lp4JAfUOv-PA"
      },
      "id": "lp4JAfUOv-PA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\")\n",
        "df = df.fillna(method=\"ffill\")\n",
        "\n",
        "# Preprocess the dataset\n",
        "class EntityDataset:\n",
        "    def __init__(self, df):\n",
        "        self.sentences = []\n",
        "        self.labels = []\n",
        "        agg_func = lambda s: list(s)\n",
        "        grouped = df.groupby(\"Sentence #\").agg({\"Word\": agg_func, \"Tag\": agg_func})\n",
        "        for _, row in grouped.iterrows():\n",
        "            self.sentences.append(row[\"Word\"])\n",
        "            self.labels.append(row[\"Tag\"])\n",
        "\n",
        "entity_data = EntityDataset(df)\n",
        "\n",
        "label_list = list(set(tag for tags in entity_data.labels for tag in tags))\n",
        "label_list.sort()\n",
        "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
        "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
        "\n",
        "data = [{\"tokens\": s, \"ner_tags\": [label_to_id[tag] for tag in tags]} for s, tags in zip(entity_data.sentences, entity_data.labels)]\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kYaKSMUYwClz"
      },
      "id": "kYaKSMUYwClz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and Model\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "model = BertForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
        "\n",
        "# Tokenization and alignment\n",
        "def tokenize_and_align_labels(example):\n",
        "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    word_ids = tokenized_inputs.word_ids()\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            labels.append(-100)\n",
        "        elif word_idx != previous_word_idx:\n",
        "            labels.append(example[\"ner_tags\"][word_idx])\n",
        "        else:\n",
        "            labels.append(-100)\n",
        "        previous_word_idx = word_idx\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=False)\n",
        "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "F6l1EEc8wIJ-"
      },
      "id": "F6l1EEc8wIJ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n"
      ],
      "metadata": {
        "id": "vRCWAQuTwLvz"
      },
      "id": "vRCWAQuTwLvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = torch.argmax(torch.tensor(predictions), dim=2)\n",
        "    true_predictions = [\n",
        "        [id_to_label[pred] for (pred, label) in zip(prediction, label) if label != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id_to_label[label] for (pred, label) in zip(prediction, label) if label != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return metric.compute(predictions=true_predictions, references=true_labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "L4qjxDtHwPn_"
      },
      "id": "L4qjxDtHwPn_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ToBfUv-swRWN"
      },
      "id": "ToBfUv-swRWN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}